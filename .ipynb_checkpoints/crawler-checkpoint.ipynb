{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pickle\n",
    "PICKLE_FILEPATHS = {'overview':'overview_htmls.pkl', 'player':'player_htmls.pkl'}\n",
    "\n",
    "def get_almost_constants():\n",
    "    with open('almost_constants.json', 'r') as f:\n",
    "        almost_constants = json.load(f)\n",
    "    return almost_constants\n",
    "\n",
    "async def fetch(session, url):\n",
    "    with aiohttp.Timeout(30):\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def fetch_all(session, urls, loop):\n",
    "    results = await asyncio.gather(\n",
    "        *[fetch(session, url) for url in urls],\n",
    "        return_exceptions=True  # so we can deal with exceptions later\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_htmls_from_pickle(file_key):\n",
    "    with open(PICKLE_FILEPATHS[file_key], 'rb') as f:\n",
    "        htmls = pickle.load(f)\n",
    "    return htmls\n",
    "\n",
    "def save_htmls_to_pickle(htmls, file_key):\n",
    "    with open(PICKLE_FILEPATHS[file_key], 'wb') as f:\n",
    "        pickle.dump(overview_htmls, f)\n",
    "    \n",
    "\n",
    "def get_htmls(urls, from_file=False, file_key=None):\n",
    "    if from_file:\n",
    "        return get_htmls_from_pickle(file_key)\n",
    "    else:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with aiohttp.ClientSession(loop=loop) as session:\n",
    "            htmls = loop.run_until_complete(fetch_all(session, urls, loop))\n",
    "    return dict(zip(urls, htmls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_overview_urls():\n",
    "    urls = []\n",
    "    base_url = \"https://sofifa.com/players?offset=\"\n",
    "    offset_increment = 80\n",
    "    for i in range(226): # WARNING: this may not be invariant\n",
    "        url = base_url + str(i * offset_increment)\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_player_urls(IDs):\n",
    "    urls = []\n",
    "    base_url = 'https://sofifa.com/player/'\n",
    "    for ID in IDs:\n",
    "        url = base_url + str(ID)\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_player_htmls(IDs, from_file=False):\n",
    "    urls = get_player_urls(IDs)\n",
    "    return get_htmls(urls, from_file, file_key='player')\n",
    "\n",
    "def id_from_url(url):\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "def get_player_soups(IDs, from_file=False):\n",
    "    strainer = SoupStrainer(['section', 'script'])\n",
    "    player_htmls = get_player_htmls(IDs, from_file)\n",
    "    return {id_from_url(url): BeautifulSoup(html, 'lxml', parse_only=strainer) for url, html in player_htmls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_overview_htmls(from_file=False):\n",
    "    urls = get_overview_urls()\n",
    "    return get_htmls(urls, from_file, file_key='overview')\n",
    "\n",
    "def get_overview_soups(from_file=False):\n",
    "    strainer = SoupStrainer('tbody')\n",
    "    htmls = get_overview_htmls(from_file)\n",
    "    return [BeautifulSoup(html, 'lxml', parse_only=strainer) for html in htmls.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%time overview_htmls = get_htmls(get_overview_urls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_htmls_to_pickle(overview_htmls, 'overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_single_row(overview_table_row):\n",
    "    \n",
    "    record_dict = {}\n",
    "    td = overview_table_row.find_all('td')\n",
    "    record_dict['photo'] = td[0].find('img').get('data-src')\n",
    "    record_dict['ID'] = td[0].find('img').get('id')\n",
    "    record_dict['nationality'] = td[1].find('a').get('title')\n",
    "    record_dict['flag'] = td[1].find('img').get('data-src')\n",
    "    record_dict['name'] = td[1].find_all('a')[1].text\n",
    "    record_dict['age'] = td[2].find('div').text.strip()\n",
    "    record_dict['overall'] = td[3].text.strip()\n",
    "    record_dict['potential'] = td[4].text.strip()\n",
    "    record_dict['club'] = td[5].find('a').text\n",
    "    record_dict['club_logo'] = td[5].find('img').get('data-src')\n",
    "    record_dict['value'] = td[7].text\n",
    "    record_dict['wage'] = td[8].text\n",
    "    record_dict['special'] = td[17].text\n",
    "    \n",
    "    return record_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_single_overview_page(soup):\n",
    "    row_dicts = []\n",
    "    for row in soup.find_all('tr'):\n",
    "        row_dicts.append(parse_single_row(row))\n",
    "    return row_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_overview_data(overview_soups):\n",
    "    data = []\n",
    "    for soup in overview_soups:\n",
    "        row_dicts = parse_single_overview_page(soup)\n",
    "        data.extend(row_dicts)\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doesn't work in notebook but should work otherwise\n",
    "# import multiprocessing as mp\n",
    "# num_workers = mp.cpu_count()\n",
    "# pool = mp.Pool(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def square(x):\n",
    "#     return x**2\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     pool.map(square, [1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# x = pool.map(parse_single_overview_page, overview_htmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_currency(curr_col):\n",
    "    without_euro_symbol = curr_col.str[1:]\n",
    "    unit_symbol = without_euro_symbol.str[-1]\n",
    "    numeric_part = np.where(unit_symbol == '0', 0, without_euro_symbol.str[:-1].pipe(pd.to_numeric))\n",
    "    multipliers = unit_symbol.replace({'M':1e6, 'K':1e3}).pipe(pd.to_numeric)\n",
    "    return numeric_part * multipliers\n",
    "\n",
    "def clean_overview_data(df):\n",
    "    return (df.assign(EUR_value = lambda df: df['value'].pipe(convert_currency), \n",
    "                                EUR_wage = lambda df: df['wage'].pipe(convert_currency))\n",
    "            .drop(['value', 'wage'], axis=1))\n",
    "\n",
    "#player_personal_data = df.pipe(clean_personal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_overview_data(from_file=False):\n",
    "    overview_soups = get_overview_soups()\n",
    "    return parse_overview_data(overview_soups).pipe(clean_overview_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_overview_data = get_overview_data(from_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#player_personal_data.to_csv('Complete/PlayerPersonalData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def headline_attribute_from_line(line):\n",
    "    equals_sign_loc = line.find('=')\n",
    "    attribute_name = line[equals_sign_loc - 4: equals_sign_loc - 1].lower()\n",
    "    attribute_value = int(line[equals_sign_loc+2:equals_sign_loc+4])\n",
    "    return {'name':attribute_name, 'value':attribute_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def standardise_spelling(player_attribute_name):\n",
    "#     return player_attribute_name.lower().replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardise_spelling(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_data_url = 'https://sofifa.com/player/20801'\n",
    "# skill_names = ['ID', 'crossing', 'finishing', 'heading_accuracy','short_passing', 'volleys', 'dribbling', 'curve',\n",
    "#                'free_kick_accuracy', 'long_passing', 'ball_control', 'acceleration', 'sprint_speed', 'agility',\n",
    "#                'reactions', 'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots', 'aggression',\n",
    "#                'interceptions', 'positioning', 'vision', 'penalties', 'composure', 'marking', 'standing_tackle',\n",
    "#                'sliding_tackle', 'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes']\n",
    "# headline_attribute_names = ['PAC', 'SHO', 'PAS', 'DRI', 'DEF', 'PHY']\n",
    "# all_attribute_names = skill_names + headline_attribute_names\n",
    "player_attribute_dict = {'ID': 20801}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_code = requests.get(player_data_url)\n",
    "gk_source_code = requests.get('https://sofifa.com/player/228736')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plain_text = source_code.text\n",
    "strainer = SoupStrainer(['section', 'script'])\n",
    "soup = BeautifulSoup(plain_text, 'lxml', parse_only=strainer)\n",
    "gk_soup = BeautifulSoup(gk_source_code.text, 'lxml', parse_only=strainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parse_col3_divs(soup):\n",
    "    col_divs = (soup.find('section', class_='container', recursive=False)\n",
    "                .section\n",
    "                .article\n",
    "                .find_all('div', class_='columns', recursive=False))\n",
    "    col3_divs = []\n",
    "    for sub_div in col_divs:\n",
    "        col3_divs.extend(sub_div.find_all('div', class_='col-3', recursive=False))\n",
    "    return col3_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_main_attributes(col3_divs):\n",
    "    attribute_dict = {}\n",
    "    for sub_div in col3_divs[:-1]: # last one is traits and specialities\n",
    "        stripped_strings = sub_div.ul.stripped_strings\n",
    "        for s in stripped_strings:\n",
    "            attribute_dict[next(stripped_strings)] = s\n",
    "    return attribute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col3_divs = _parse_col3_divs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%time x = parse_main_attributes(col3_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headline attributes like PHY: seems to be related to Ultimate Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_headline_attributes(soup):\n",
    "    attribute_dict = {}\n",
    "    headline_attribute_script = soup.find_all('script')[1]\n",
    "    for line in headline_attribute_script.text.split('\\r\\n'):\n",
    "        if 'point' in line:\n",
    "            attr_subdict = headline_attribute_from_line(line)\n",
    "            attribute_dict[attr_subdict['name']] = attr_subdict['value']\n",
    "    return attribute_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta section at top of player page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_player_metadata(soup):\n",
    "    \n",
    "    attribute_dict = {}\n",
    "    player_info_html = soup.find('div', class_='meta').find('span')\n",
    "    # nationality, age and flag were found in player overview\n",
    "    attribute_dict['preferred_positions'] = [span.text for span in player_info_html.find_all('span')]\n",
    "    age_height_weight = player_info_html.contents[-1].split()\n",
    "    attribute_dict['Birth date'] = ' '.join(age_height_weight[2:5]).replace(',', '').strip('(').strip(')')\n",
    "    attribute_dict['Height_cm'] = age_height_weight[5].strip('cm')\n",
    "    attribute_dict['Weight_kg'] = age_height_weight[-1].strip('kg')\n",
    "    \n",
    "    return attribute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_uls = soup.find_all('ul', class_='pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Power free kick', 'Flair', 'Long shot taker', 'Skilled dribbling']"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_standardise_ul(col3_divs[-1].div.ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"pl\">\n",
       "<li>Speedster</li>\n",
       "<li>Dribbler</li>\n",
       "<li>Distance shooter</li>\n",
       "<li>Acrobat</li>\n",
       "<li>Clinical finisher</li>\n",
       "<li>Complete forward</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col3_divs[-1].div.find_all('ul')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _standardise_ul(ul):\n",
    "    return list(standardise_spelling(item) for item in ul.stripped_strings)\n",
    "\n",
    "def _get_traits_and_specialities_dict(player_traits, player_specialities, all_traits, all_specialities):\n",
    "    trait_dict = {trait: (trait in player_traits) for trait in all_traits}\n",
    "    speciality_dict = {speciality: (speciality in specialities) for speciality in all_specialities}\n",
    "    return {**trait_dict, **speciality_dict}\n",
    "\n",
    "def parse_traits_and_specialities(col3_divs, all_traits, all_specialities):\n",
    "    uls = col3_divs[-1].div.find_all('ul')\n",
    "    player_traits = _standardise_ul(uls[0])\n",
    "    player_specialities = _standardise_ul(page_uls[1])\n",
    "    result = _get_traits_and_specialities_dict(player_traits, player_specialities, all_traits, all_specialities)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_player_miscellaneous_data(page_uls):\n",
    "    data = page_uls[0]\n",
    "    attribute_dict = {}\n",
    "    strings = data.stripped_strings\n",
    "    for key in strings:\n",
    "        attribute_dict[standardise_spelling(key)] = next(strings)\n",
    "    work_rates = attribute_dict.pop('Work rate').split(' / ')\n",
    "    attribute_dict['Work rate att'] = work_rates[0]\n",
    "    attribute_dict['work rate def'] = work_rates[1]\n",
    "    return attribute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_position_ratings(soup, all_positions):\n",
    "    position_col_name = 'Position'\n",
    "    ratings_table = soup.find('table', class_='table table-hover')\n",
    "    if ratings_table:\n",
    "        position_ratings_df = (pd.read_html(str(ratings_table))[0][[position_col_name, 'OVA']]\n",
    "                        .rename(columns=standardise_spelling))\n",
    "        split_df = (position_ratings_df[position_col_name]\n",
    "                    .str.split(expand=True)\n",
    "                    .assign(ova=p['ova']))\n",
    "        position_ratings_dict = (pd.concat(split_df[[i, 'ova']].rename(columns={i:position_col_name}) for i in range(3))\n",
    "                                 .dropna()\n",
    "                                 .set_index(position_col_name)\n",
    "                                 .to_dict()['ova'])\n",
    "        position_ratings_dict.update({'GK':np.nan})\n",
    "    else:\n",
    "        gk_rating = soup.find('div', class_='stats').td.span.text\n",
    "        position_ratings_dict = {'GK':gk_rating, **{pos:np.nan for pos in all_positions}}\n",
    "    return position_ratings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this isn't actually any better than hard-coding as these positions are invariant\n",
    "# def get_unique_positions(position_ratings):\n",
    "#     return position_ratings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put in separate script\n",
    "# def get_all_traits_and_specialities():\n",
    "#     url = 'https://sofifa.com/players/top'\n",
    "#     html = requests.get(url).text\n",
    "#     strainer = SoupStrainer('form', action='/players', class_='pjax relative')\n",
    "#     soup = BeautifulSoup(html, 'lxml', parse_only=strainer)\n",
    "#     traits1 = list(standardise_spelling(item) for item in traits_soup.find(attrs={'name':'t1[]'}).stripped_strings)\n",
    "#     traits2 = list(standardise_spelling(item) for item in traits_soup.find(attrs={'name':'t2[]'}).stripped_strings)\n",
    "#     all_traits = [*traits1, *traits2]\n",
    "#     all_specialities = list(standardise_spelling(item) for item in traits_soup.find(attrs={'name':'sc[]'}).stripped_strings)\n",
    "#     return {'traits':all_traits, 'specialities':all_specialities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x = get_all_traits_and_specialities()\n",
    "\n",
    "# import json\n",
    "# almost_constants = {**x, 'positions':unique_positions}\n",
    "# with open('almost_constants.json', 'w') as f:\n",
    "#     json.dump(almost_constants, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_full_position_preferences(preferred_positions_list, all_positions):\n",
    "    return {'prefers_' + pos: (pos in preferred_positions_list) for pos in all_positions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_single_player_page(soup, almost_constants):\n",
    "    \n",
    "    all_traits = almost_constants['traits']\n",
    "    all_specialities = almost_constants['specialities']\n",
    "    all_positions = almost_constants['positions']\n",
    "    \n",
    "    col3_divs = _parse_col3_divs(soup)\n",
    "    main_attributes = parse_main_attributes(col3_divs)\n",
    "    headline_attributes = parse_headline_attributes(soup)\n",
    "    metadata = parse_player_metadata(soup)\n",
    "    _preferred_positions = metadata.pop('preferred_positions')\n",
    "    _page_uls = soup.find_all('ul', class_='pl')\n",
    "    traits_and_specialities = parse_traits_and_specialities(_page_uls, all_traits, all_specialities)\n",
    "    miscellaneous_data = parse_player_miscellaneous_data(_page_uls)\n",
    "    position_ratings = get_position_ratings(soup, all_positions)\n",
    "    position_preferences = get_full_position_preferences(_preferred_positions, all_positions)\n",
    "    return {**main_attributes, **headline_attributes, **metadata, \n",
    "            **traits_and_specialities, **miscellaneous_data, **position_ratings,\n",
    "           **position_preferences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_player_detailed_data(player_soups, almost_constants):\n",
    "    data = []\n",
    "    for player_id, soup in player_soups.items():\n",
    "        row_dict = parse_single_player_page(soup, almost_constants)\n",
    "        row_dict['ID'] = player_id\n",
    "        data.append(row_dict)\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_player_detailed_data(IDs, from_file=False):\n",
    "    almost_constants = get_almost_constants()\n",
    "    player_soups = get_player_soups(IDs, from_file)\n",
    "    return parse_player_detailed_data(player_soups, almost_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun -s \"cumulative\" z = parse_single_player_page(soup, almost_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time z = parse_single_player_page(soup, almost_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Kevin\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "test_IDs = player_overview_data['ID'].head(10)\n",
    "player_soups = get_player_soups(test_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(from_file):\n",
    "    # download overview htmls. Parse these into a dataframe and save this into a variable.\n",
    "    # Use the df's ID column to get urls for player personal data.\n",
    "    # for the first player url only, use the position ratings table to get a sequence of unique positions, and save this as a variable\n",
    "    # actually just hard code it\n",
    "    player_overview_data = get_overview_data(from_file)\n",
    "    IDs = player_overview_data['ID']\n",
    "    player_detailed_data = get_player_detailed_data(IDs, from_file)\n",
    "    merged = player_overview_data.merge(player_detailed_data, on='ID')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data.to_csv('Allplayer.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_data.to_csv('Complete/PlayerAttributeData.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data.to_csv('Complete/Dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data.drop('Unnamed: 0', 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data.drop('ID_x', 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['ID_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = full_data.rename(index=str, columns={\"ID_y\": \"ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.to_csv('Complete/Dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heights = ['1cm' for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 694 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = pd.Series(heights)\n",
    "s2 = s.str.strip('cm').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "heights2 = [item.strip('cm') for item in heights]\n",
    "s2 = pd.Series(heights2).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".strip method is faster on individual strings in a loop than in pandas Series, for some reason. Type conversion from str to int is still faster with Series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
