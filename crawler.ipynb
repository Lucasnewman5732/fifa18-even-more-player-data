{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'crawler.overview_data' from 'C:\\\\Users\\\\Kevin\\\\Documents\\\\GitHub\\\\fifa18-all-player-statistics\\\\crawler\\\\overview_data.py'>"
      ]
     },
     "execution_count": 1388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(crawler.overview_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html.update_pickled_overview_htmls(overview_htmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawler.overview_data import get_overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%time overview_htmls = html.get_overview_htmls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%time y = get_overview_data(from_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.3 s\n"
     ]
    }
   ],
   "source": [
    "%time overview_htmls = get_overview_htmls(from_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_htmls_to_pickle(overview_htmls, 'overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script_strainer = SoupStrainer('script')\n",
    "script_url = 'https://sofifa.com/player/158023'\n",
    "script_html = requests.get(script_url).text\n",
    "script_soup = BeautifulSoup(script_html, 'lxml', parse_only=script_strainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1362,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<script>\n",
       "var playerId = 158023;\n",
       "var playerHistoryUrl = \"/ajax.php?action=history&type=player&id=158023&version=18&export=158879\";\n",
       "var overallRating = 'Overall rating';\n",
       "var potential = 'Potential';\n",
       "var labelPAC = 'PAC';\n",
       "var labelSHO = 'SHO';\n",
       "var labelPAS = 'PAS';\n",
       "var labelDRI = 'DRI';\n",
       "var labelDEF = 'DEF';\n",
       "var labelPHY = 'PHY';\n",
       "var pointPAC = 89;\n",
       "var pointSHO = 90;\n",
       "var pointPAS = 86;\n",
       "var pointDRI = 96;\n",
       "var pointDEF = 26;\n",
       "var pointPHY = 61;\n",
       "var version = '18';\n",
       "var exportdate = '158879';\n",
       "var position = 23;\n",
       "</script>"
      ]
     },
     "execution_count": 1362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_soup.find_all('script', recursive=False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_data_url = 'https://sofifa.com/player/20801'\n",
    "# skill_names = ['ID', 'crossing', 'finishing', 'heading_accuracy','short_passing', 'volleys', 'dribbling', 'curve',\n",
    "#                'free_kick_accuracy', 'long_passing', 'ball_control', 'acceleration', 'sprint_speed', 'agility',\n",
    "#                'reactions', 'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots', 'aggression',\n",
    "#                'interceptions', 'positioning', 'vision', 'penalties', 'composure', 'marking', 'standing_tackle',\n",
    "#                'sliding_tackle', 'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes']\n",
    "# headline_attribute_names = ['PAC', 'SHO', 'PAS', 'DRI', 'DEF', 'PHY']\n",
    "# all_attribute_names = skill_names + headline_attribute_names\n",
    "player_attribute_dict = {'ID': 20801}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_code = requests.get(player_data_url)\n",
    "gk_source_code = requests.get('https://sofifa.com/player/228736')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65.5 ms\n"
     ]
    }
   ],
   "source": [
    "plain_text = source_code.text\n",
    "strainer = SoupStrainer(['section', 'script'])\n",
    "soup = BeautifulSoup(plain_text, 'lxml', parse_only=strainer)\n",
    "gk_soup = BeautifulSoup(gk_source_code.text, 'lxml', parse_only=strainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_main_soup(soup):\n",
    "    return soup.find_all('section', recursive=False)[2]\n",
    "\n",
    "def _get_main_article(main_soup):\n",
    "    return main_soup.section.article\n",
    "\n",
    "def _get_col3_divs(main_article):\n",
    "    col_divs = (main_article\n",
    "                .find_all('div', class_='columns', recursive=False))\n",
    "    col3_divs = []\n",
    "    for sub_div in col_divs:\n",
    "        col3_divs.extend(sub_div.find_all('div', class_='col-3', recursive=False))\n",
    "    return col3_divs\n",
    "\n",
    "def parse_main_attributes(col3_divs):\n",
    "    attribute_dict = {}\n",
    "    for sub_div in col3_divs[:-1]: # last one is traits and specialities\n",
    "        for li in sub_div.div.ul.find_all('li', recursive=False):\n",
    "            stripped_strings = list(li.stripped_strings)\n",
    "            attribute_name = stripped_strings[-1]\n",
    "            attribute_value = stripped_strings[0]\n",
    "            attribute_dict[attribute_name] = attribute_value\n",
    "    return attribute_dict\n",
    "\n",
    "def parse_player_metadata(main_article):\n",
    "    \n",
    "    attribute_dict = {}\n",
    "    player_info_soup = main_article.div.div.div\n",
    "    stripped_strings = list(player_info_soup.span.stripped_strings)\n",
    "    attribute_dict['preferred_positions'] = stripped_strings[1:-1]\n",
    "    age_height_weight = stripped_strings[-1].split()\n",
    "    attribute_dict['Birth date'] = ' '.join(age_height_weight[2:5]).replace(',', '').strip('(').strip(')')\n",
    "    attribute_dict['Height_cm'] = age_height_weight[5].strip('cm')\n",
    "    attribute_dict['Weight_kg'] = age_height_weight[-1].strip('kg')\n",
    "    \n",
    "    return attribute_dict\n",
    "\n",
    "def _get_traits_and_specialities_dict(player_traits, player_specialities, all_traits, all_specialities):\n",
    "    trait_dict = {trait: (trait in player_traits) for trait in all_traits}\n",
    "    speciality_dict = {speciality: (speciality in specialities) for speciality in all_specialities}\n",
    "    return {**trait_dict, **speciality_dict}\n",
    "\n",
    "def parse_traits_and_specialities(col3_divs, all_traits, all_specialities):\n",
    "    last_div = col3_divs[-1]\n",
    "    if not last_div.text.strip():\n",
    "        player_traits, player_specialities = [np.nan], [np.nan]\n",
    "    else:\n",
    "        uls = last_div.div.find_all('ul', recursive=False)\n",
    "        n_uls = len(uls)\n",
    "        if n_uls == 1:\n",
    "            ul = uls[0]\n",
    "            ul_strings = list(ul.stripped_strings)\n",
    "            ul_h5 = ul.parent.h5.text\n",
    "            if ul_h5 == 'Traits':\n",
    "                player_traits = ul_strings\n",
    "                player_specialities = [np.nan]\n",
    "            elif ul_h5 == 'Specialities':\n",
    "                player_traits = [np.nan]\n",
    "                player_specialities = ul_strings\n",
    "        else:\n",
    "            player_traits = list(uls[0].stripped_strings)\n",
    "            player_specialities = list(uls[1].stripped_strings)\n",
    "    result = _get_traits_and_specialities_dict(player_traits, player_specialities, all_traits, all_specialities)\n",
    "    return result\n",
    "\n",
    "def parse_player_miscellaneous_data(main_article):\n",
    "    ul = (main_article.div\n",
    "          .find('div', class_='teams', recursive=False)\n",
    "          .table.tr.ul)\n",
    "    attribute_dict = {}\n",
    "    strings = ul.stripped_strings\n",
    "    for key in strings:\n",
    "        attribute_dict[key] = next(strings)\n",
    "    work_rates = attribute_dict.pop('Work rate').split(' / ')\n",
    "    attribute_dict['Work rate att'] = work_rates[0]\n",
    "    attribute_dict['Work rate def'] = work_rates[1]\n",
    "    return attribute_dict\n",
    "\n",
    "def get_position_ratings(main_soup, main_article, all_positions):\n",
    "    position_col_name = 'Position'\n",
    "    ratings_div = main_soup.aside.find('div', class_='toast mb-20', recursive=False)\n",
    "    if ratings_div.h5.text == 'Real overall rating':\n",
    "        ratings_table = ratings_div.table\n",
    "        position_ratings_df = pd.read_html(str(ratings_table))[0][[position_col_name, 'OVA']]\n",
    "        split_df = (position_ratings_df[position_col_name]\n",
    "                    .str.split(expand=True)\n",
    "                    .assign(ova=p['ova']))\n",
    "        position_ratings_dict = (pd.concat(split_df[[i, 'ova']].rename(columns={i:position_col_name}) for i in range(3))\n",
    "                                 .dropna()\n",
    "                                 .set_index(position_col_name)\n",
    "                                 .to_dict()['ova'])\n",
    "        position_ratings_dict.update({'GK':np.nan})\n",
    "    else:\n",
    "        gk_rating = main_article.div.find('div', class_='stats', recursive=False).td.span.text\n",
    "        position_ratings_dict = {'GK':gk_rating, **{pos:np.nan for pos in all_positions}}\n",
    "    return position_ratings_dict\n",
    "\n",
    "def get_full_position_preferences(preferred_positions_list, all_positions):\n",
    "    return {'prefers_' + pos: (pos in preferred_positions_list) for pos in all_positions}\n",
    "\n",
    "def parse_single_player_page(html, strainer, constants):\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'lxml', parse_only=strainer)\n",
    "    all_traits = constants['traits']\n",
    "    all_specialities = constants['specialities']\n",
    "    all_positions = constants['positions']\n",
    "    \n",
    "    main_soup = _get_main_soup(soup)\n",
    "    main_article = _get_main_article(main_soup)\n",
    "    col3_divs = _get_col3_divs(main_article)\n",
    "    main_attributes = parse_main_attributes(col3_divs)\n",
    "    headline_attributes = parse_headline_attributes(soup)\n",
    "    metadata = parse_player_metadata(main_article)\n",
    "    _preferred_positions = metadata.pop('preferred_positions')\n",
    "    traits_and_specialities = parse_traits_and_specialities(col3_divs, all_traits, all_specialities)\n",
    "    miscellaneous_data = parse_player_miscellaneous_data(main_article)\n",
    "    position_ratings = get_position_ratings(main_soup, main_article, all_positions)\n",
    "    position_preferences = get_full_position_preferences(_preferred_positions, all_positions)\n",
    "    return {**main_attributes, **headline_attributes, **metadata, \n",
    "            **traits_and_specialities, **miscellaneous_data, **position_ratings,\n",
    "           **position_preferences}\n",
    "\n",
    "def id_from_url(url):\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "def parse_player_detailed_data(player_htmls, constants):\n",
    "    strainer = SoupStrainer(['section', 'script'])\n",
    "    data = []\n",
    "    for player_id, html in player_htmls.items():\n",
    "        row_dict = parse_single_player_page(html, strainer, constants)\n",
    "        row_dict['ID'] = id_from_url(player_id)\n",
    "        data.append(row_dict)\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def get_player_detailed_data(IDs, from_file=False):\n",
    "    constants = read_constants()\n",
    "    player_htmls = get_player_htmls(IDs, from_file)\n",
    "    return parse_player_detailed_data(player_soups, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_IDs = player_overview_data['ID'].sample(50)\n",
    "player_htmls = get_player_htmls(test_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = parse_player_detailed_data(player_htmls, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(from_file=False):\n",
    "    player_overview_data = get_overview_data(from_file)\n",
    "    IDs = player_overview_data['ID']\n",
    "    player_detailed_data = get_player_detailed_data(IDs, from_file)\n",
    "    merged = player_overview_data.merge(player_detailed_data, on='ID')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heights = ['1cm' for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 694 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = pd.Series(heights)\n",
    "s2 = s.str.strip('cm').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "heights2 = [item.strip('cm') for item in heights]\n",
    "s2 = pd.Series(heights2).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".strip method is faster on individual strings in a loop than in pandas Series, for some reason. Type conversion from str to int is still faster with Series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
